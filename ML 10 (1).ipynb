{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n","    # Clip y_pred to prevent log(0)\n","    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n","    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"],"metadata":{"id":"5Y48xQEGej7V","executionInfo":{"status":"ok","timestamp":1744306774611,"user_tz":-330,"elapsed":20,"user":{"displayName":"Onkar Jadhav","userId":"00426842055970906743"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def train_neural_network(X, y, epochs=1000, lr=0.1):\n","    input_dim = X.shape[1]\n","    weights = np.random.uniform(size=(input_dim, 1))\n","    bias = np.random.uniform(size=(1,))\n","\n","    for epoch in range(epochs):\n","        # Forward pass\n","        linear_output = np.dot(X, weights) + bias\n","        predictions = sigmoid(linear_output)\n","\n","        # Compute loss\n","        loss = binary_cross_entropy(y, predictions)\n","\n","        # Backpropagation\n","        error = predictions - y\n","        d_pred = error * sigmoid_derivative(predictions)\n","\n","        # Gradient descent update\n","        weights -= lr * np.dot(X.T, d_pred)\n","        bias -= lr * np.sum(d_pred)\n","\n","        # Print loss every 100 epochs\n","        if epoch % 100 == 0:\n","            print(f\"Epoch {epoch}: Loss {loss:.4f}\")\n","\n","    return weights, bias\n"],"metadata":{"id":"F4Qc2mBtektL","executionInfo":{"status":"ok","timestamp":1744306774614,"user_tz":-330,"elapsed":21,"user":{"displayName":"Onkar Jadhav","userId":"00426842055970906743"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def predict(X, weights, bias):\n","    linear_output = np.dot(X, weights) + bias\n","    probabilities = sigmoid(linear_output)\n","    return (probabilities >= 0.5).astype(int)"],"metadata":{"id":"yKVcIKiehkU9","executionInfo":{"status":"ok","timestamp":1744306774643,"user_tz":-330,"elapsed":6,"user":{"displayName":"Onkar Jadhav","userId":"00426842055970906743"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["X = np.array([[0,0], [0,1], [1,0], [1,1]])\n","\n","logic_gates = {\n","  \"AND\": np.array([[0], [0], [0], [1]]),\n","  \"OR\" : np.array([[0], [1], [1], [1]]),\n","  \"NAND\": np.array([[1], [1], [1], [0]]),\n","  \"NOR\": np.array([[1], [0], [0], [0]]),\n","  \"XOR\": np.array([[0], [1], [1], [0]])\n","}\n","\n","for gate_name, y in logic_gates.items():\n","    print(f\"\\nTraining {gate_name} gate:\")\n","    weights, bias = train_neural_network(X, y, epochs=10000, lr=0.1)\n","    predictions = predict(X, weights, bias).astype(int)\n","    print(f\"Predictions for {gate_name} gate: \\n{predictions.reshape(-1)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKfZ9C3Thw6T","executionInfo":{"status":"ok","timestamp":1744306777406,"user_tz":-330,"elapsed":2764,"user":{"displayName":"Onkar Jadhav","userId":"00426842055970906743"}},"outputId":"58c99081-9a43-418d-d9b0-a3a6483ee1b4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training AND gate:\n","Epoch 0: Loss 0.9081\n","Epoch 100: Loss 0.4743\n","Epoch 200: Loss 0.3831\n","Epoch 300: Loss 0.3269\n","Epoch 400: Loss 0.2879\n","Epoch 500: Loss 0.2588\n","Epoch 600: Loss 0.2360\n","Epoch 700: Loss 0.2176\n","Epoch 800: Loss 0.2024\n","Epoch 900: Loss 0.1896\n","Epoch 1000: Loss 0.1786\n","Epoch 1100: Loss 0.1691\n","Epoch 1200: Loss 0.1608\n","Epoch 1300: Loss 0.1534\n","Epoch 1400: Loss 0.1468\n","Epoch 1500: Loss 0.1410\n","Epoch 1600: Loss 0.1356\n","Epoch 1700: Loss 0.1308\n","Epoch 1800: Loss 0.1264\n","Epoch 1900: Loss 0.1224\n","Epoch 2000: Loss 0.1186\n","Epoch 2100: Loss 0.1152\n","Epoch 2200: Loss 0.1120\n","Epoch 2300: Loss 0.1091\n","Epoch 2400: Loss 0.1063\n","Epoch 2500: Loss 0.1037\n","Epoch 2600: Loss 0.1013\n","Epoch 2700: Loss 0.0990\n","Epoch 2800: Loss 0.0969\n","Epoch 2900: Loss 0.0949\n","Epoch 3000: Loss 0.0929\n","Epoch 3100: Loss 0.0911\n","Epoch 3200: Loss 0.0894\n","Epoch 3300: Loss 0.0878\n","Epoch 3400: Loss 0.0862\n","Epoch 3500: Loss 0.0847\n","Epoch 3600: Loss 0.0833\n","Epoch 3700: Loss 0.0820\n","Epoch 3800: Loss 0.0807\n","Epoch 3900: Loss 0.0794\n","Epoch 4000: Loss 0.0782\n","Epoch 4100: Loss 0.0771\n","Epoch 4200: Loss 0.0760\n","Epoch 4300: Loss 0.0749\n","Epoch 4400: Loss 0.0739\n","Epoch 4500: Loss 0.0729\n","Epoch 4600: Loss 0.0720\n","Epoch 4700: Loss 0.0711\n","Epoch 4800: Loss 0.0702\n","Epoch 4900: Loss 0.0694\n","Epoch 5000: Loss 0.0685\n","Epoch 5100: Loss 0.0677\n","Epoch 5200: Loss 0.0670\n","Epoch 5300: Loss 0.0662\n","Epoch 5400: Loss 0.0655\n","Epoch 5500: Loss 0.0648\n","Epoch 5600: Loss 0.0641\n","Epoch 5700: Loss 0.0635\n","Epoch 5800: Loss 0.0628\n","Epoch 5900: Loss 0.0622\n","Epoch 6000: Loss 0.0616\n","Epoch 6100: Loss 0.0610\n","Epoch 6200: Loss 0.0604\n","Epoch 6300: Loss 0.0598\n","Epoch 6400: Loss 0.0593\n","Epoch 6500: Loss 0.0588\n","Epoch 6600: Loss 0.0582\n","Epoch 6700: Loss 0.0577\n","Epoch 6800: Loss 0.0572\n","Epoch 6900: Loss 0.0568\n","Epoch 7000: Loss 0.0563\n","Epoch 7100: Loss 0.0558\n","Epoch 7200: Loss 0.0554\n","Epoch 7300: Loss 0.0549\n","Epoch 7400: Loss 0.0545\n","Epoch 7500: Loss 0.0541\n","Epoch 7600: Loss 0.0537\n","Epoch 7700: Loss 0.0533\n","Epoch 7800: Loss 0.0529\n","Epoch 7900: Loss 0.0525\n","Epoch 8000: Loss 0.0521\n","Epoch 8100: Loss 0.0517\n","Epoch 8200: Loss 0.0514\n","Epoch 8300: Loss 0.0510\n","Epoch 8400: Loss 0.0507\n","Epoch 8500: Loss 0.0503\n","Epoch 8600: Loss 0.0500\n","Epoch 8700: Loss 0.0497\n","Epoch 8800: Loss 0.0493\n","Epoch 8900: Loss 0.0490\n","Epoch 9000: Loss 0.0487\n","Epoch 9100: Loss 0.0484\n","Epoch 9200: Loss 0.0481\n","Epoch 9300: Loss 0.0478\n","Epoch 9400: Loss 0.0475\n","Epoch 9500: Loss 0.0472\n","Epoch 9600: Loss 0.0469\n","Epoch 9700: Loss 0.0467\n","Epoch 9800: Loss 0.0464\n","Epoch 9900: Loss 0.0461\n","Predictions for AND gate: \n","[0 0 0 1]\n","\n","Training OR gate:\n","Epoch 0: Loss 0.4155\n","Epoch 100: Loss 0.3344\n","Epoch 200: Loss 0.2763\n","Epoch 300: Loss 0.2350\n","Epoch 400: Loss 0.2050\n","Epoch 500: Loss 0.1825\n","Epoch 600: Loss 0.1650\n","Epoch 700: Loss 0.1511\n","Epoch 800: Loss 0.1398\n","Epoch 900: Loss 0.1304\n","Epoch 1000: Loss 0.1224\n","Epoch 1100: Loss 0.1156\n","Epoch 1200: Loss 0.1097\n","Epoch 1300: Loss 0.1045\n","Epoch 1400: Loss 0.0999\n","Epoch 1500: Loss 0.0958\n","Epoch 1600: Loss 0.0921\n","Epoch 1700: Loss 0.0888\n","Epoch 1800: Loss 0.0858\n","Epoch 1900: Loss 0.0830\n","Epoch 2000: Loss 0.0805\n","Epoch 2100: Loss 0.0781\n","Epoch 2200: Loss 0.0760\n","Epoch 2300: Loss 0.0740\n","Epoch 2400: Loss 0.0721\n","Epoch 2500: Loss 0.0704\n","Epoch 2600: Loss 0.0687\n","Epoch 2700: Loss 0.0672\n","Epoch 2800: Loss 0.0658\n","Epoch 2900: Loss 0.0644\n","Epoch 3000: Loss 0.0631\n","Epoch 3100: Loss 0.0619\n","Epoch 3200: Loss 0.0608\n","Epoch 3300: Loss 0.0597\n","Epoch 3400: Loss 0.0586\n","Epoch 3500: Loss 0.0576\n","Epoch 3600: Loss 0.0567\n","Epoch 3700: Loss 0.0558\n","Epoch 3800: Loss 0.0549\n","Epoch 3900: Loss 0.0541\n","Epoch 4000: Loss 0.0533\n","Epoch 4100: Loss 0.0525\n","Epoch 4200: Loss 0.0518\n","Epoch 4300: Loss 0.0511\n","Epoch 4400: Loss 0.0504\n","Epoch 4500: Loss 0.0498\n","Epoch 4600: Loss 0.0491\n","Epoch 4700: Loss 0.0485\n","Epoch 4800: Loss 0.0479\n","Epoch 4900: Loss 0.0474\n","Epoch 5000: Loss 0.0468\n","Epoch 5100: Loss 0.0463\n","Epoch 5200: Loss 0.0458\n","Epoch 5300: Loss 0.0453\n","Epoch 5400: Loss 0.0448\n","Epoch 5500: Loss 0.0443\n","Epoch 5600: Loss 0.0439\n","Epoch 5700: Loss 0.0434\n","Epoch 5800: Loss 0.0430\n","Epoch 5900: Loss 0.0426\n","Epoch 6000: Loss 0.0422\n","Epoch 6100: Loss 0.0418\n","Epoch 6200: Loss 0.0414\n","Epoch 6300: Loss 0.0410\n","Epoch 6400: Loss 0.0406\n","Epoch 6500: Loss 0.0403\n","Epoch 6600: Loss 0.0399\n","Epoch 6700: Loss 0.0396\n","Epoch 6800: Loss 0.0393\n","Epoch 6900: Loss 0.0389\n","Epoch 7000: Loss 0.0386\n","Epoch 7100: Loss 0.0383\n","Epoch 7200: Loss 0.0380\n","Epoch 7300: Loss 0.0377\n","Epoch 7400: Loss 0.0374\n","Epoch 7500: Loss 0.0371\n","Epoch 7600: Loss 0.0369\n","Epoch 7700: Loss 0.0366\n","Epoch 7800: Loss 0.0363\n","Epoch 7900: Loss 0.0361\n","Epoch 8000: Loss 0.0358\n","Epoch 8100: Loss 0.0356\n","Epoch 8200: Loss 0.0353\n","Epoch 8300: Loss 0.0351\n","Epoch 8400: Loss 0.0349\n","Epoch 8500: Loss 0.0346\n","Epoch 8600: Loss 0.0344\n","Epoch 8700: Loss 0.0342\n","Epoch 8800: Loss 0.0340\n","Epoch 8900: Loss 0.0337\n","Epoch 9000: Loss 0.0335\n","Epoch 9100: Loss 0.0333\n","Epoch 9200: Loss 0.0331\n","Epoch 9300: Loss 0.0329\n","Epoch 9400: Loss 0.0327\n","Epoch 9500: Loss 0.0325\n","Epoch 9600: Loss 0.0323\n","Epoch 9700: Loss 0.0322\n","Epoch 9800: Loss 0.0320\n","Epoch 9900: Loss 0.0318\n","Predictions for OR gate: \n","[0 1 1 1]\n","\n","Training NAND gate:\n","Epoch 0: Loss 0.7249\n","Epoch 100: Loss 0.5255\n","Epoch 200: Loss 0.4122\n","Epoch 300: Loss 0.3458\n","Epoch 400: Loss 0.3013\n","Epoch 500: Loss 0.2690\n","Epoch 600: Loss 0.2441\n","Epoch 700: Loss 0.2242\n","Epoch 800: Loss 0.2079\n","Epoch 900: Loss 0.1942\n","Epoch 1000: Loss 0.1826\n","Epoch 1100: Loss 0.1726\n","Epoch 1200: Loss 0.1638\n","Epoch 1300: Loss 0.1561\n","Epoch 1400: Loss 0.1493\n","Epoch 1500: Loss 0.1431\n","Epoch 1600: Loss 0.1376\n","Epoch 1700: Loss 0.1326\n","Epoch 1800: Loss 0.1280\n","Epoch 1900: Loss 0.1239\n","Epoch 2000: Loss 0.1200\n","Epoch 2100: Loss 0.1165\n","Epoch 2200: Loss 0.1132\n","Epoch 2300: Loss 0.1102\n","Epoch 2400: Loss 0.1073\n","Epoch 2500: Loss 0.1047\n","Epoch 2600: Loss 0.1022\n","Epoch 2700: Loss 0.0999\n","Epoch 2800: Loss 0.0977\n","Epoch 2900: Loss 0.0956\n","Epoch 3000: Loss 0.0937\n","Epoch 3100: Loss 0.0918\n","Epoch 3200: Loss 0.0900\n","Epoch 3300: Loss 0.0884\n","Epoch 3400: Loss 0.0868\n","Epoch 3500: Loss 0.0853\n","Epoch 3600: Loss 0.0838\n","Epoch 3700: Loss 0.0825\n","Epoch 3800: Loss 0.0811\n","Epoch 3900: Loss 0.0799\n","Epoch 4000: Loss 0.0787\n","Epoch 4100: Loss 0.0775\n","Epoch 4200: Loss 0.0764\n","Epoch 4300: Loss 0.0753\n","Epoch 4400: Loss 0.0743\n","Epoch 4500: Loss 0.0733\n","Epoch 4600: Loss 0.0724\n","Epoch 4700: Loss 0.0714\n","Epoch 4800: Loss 0.0705\n","Epoch 4900: Loss 0.0697\n","Epoch 5000: Loss 0.0688\n","Epoch 5100: Loss 0.0680\n","Epoch 5200: Loss 0.0673\n","Epoch 5300: Loss 0.0665\n","Epoch 5400: Loss 0.0658\n","Epoch 5500: Loss 0.0651\n","Epoch 5600: Loss 0.0644\n","Epoch 5700: Loss 0.0637\n","Epoch 5800: Loss 0.0631\n","Epoch 5900: Loss 0.0624\n","Epoch 6000: Loss 0.0618\n","Epoch 6100: Loss 0.0612\n","Epoch 6200: Loss 0.0606\n","Epoch 6300: Loss 0.0601\n","Epoch 6400: Loss 0.0595\n","Epoch 6500: Loss 0.0590\n","Epoch 6600: Loss 0.0584\n","Epoch 6700: Loss 0.0579\n","Epoch 6800: Loss 0.0574\n","Epoch 6900: Loss 0.0569\n","Epoch 7000: Loss 0.0565\n","Epoch 7100: Loss 0.0560\n","Epoch 7200: Loss 0.0555\n","Epoch 7300: Loss 0.0551\n","Epoch 7400: Loss 0.0547\n","Epoch 7500: Loss 0.0542\n","Epoch 7600: Loss 0.0538\n","Epoch 7700: Loss 0.0534\n","Epoch 7800: Loss 0.0530\n","Epoch 7900: Loss 0.0526\n","Epoch 8000: Loss 0.0523\n","Epoch 8100: Loss 0.0519\n","Epoch 8200: Loss 0.0515\n","Epoch 8300: Loss 0.0512\n","Epoch 8400: Loss 0.0508\n","Epoch 8500: Loss 0.0505\n","Epoch 8600: Loss 0.0501\n","Epoch 8700: Loss 0.0498\n","Epoch 8800: Loss 0.0495\n","Epoch 8900: Loss 0.0491\n","Epoch 9000: Loss 0.0488\n","Epoch 9100: Loss 0.0485\n","Epoch 9200: Loss 0.0482\n","Epoch 9300: Loss 0.0479\n","Epoch 9400: Loss 0.0476\n","Epoch 9500: Loss 0.0473\n","Epoch 9600: Loss 0.0470\n","Epoch 9700: Loss 0.0468\n","Epoch 9800: Loss 0.0465\n","Epoch 9900: Loss 0.0462\n","Predictions for NAND gate: \n","[1 1 1 0]\n","\n","Training NOR gate:\n","Epoch 0: Loss 1.4667\n","Epoch 100: Loss 0.3951\n","Epoch 200: Loss 0.3128\n","Epoch 300: Loss 0.2603\n","Epoch 400: Loss 0.2234\n","Epoch 500: Loss 0.1964\n","Epoch 600: Loss 0.1758\n","Epoch 700: Loss 0.1598\n","Epoch 800: Loss 0.1469\n","Epoch 900: Loss 0.1363\n","Epoch 1000: Loss 0.1274\n","Epoch 1100: Loss 0.1199\n","Epoch 1200: Loss 0.1134\n","Epoch 1300: Loss 0.1078\n","Epoch 1400: Loss 0.1028\n","Epoch 1500: Loss 0.0984\n","Epoch 1600: Loss 0.0945\n","Epoch 1700: Loss 0.0909\n","Epoch 1800: Loss 0.0877\n","Epoch 1900: Loss 0.0848\n","Epoch 2000: Loss 0.0821\n","Epoch 2100: Loss 0.0796\n","Epoch 2200: Loss 0.0774\n","Epoch 2300: Loss 0.0753\n","Epoch 2400: Loss 0.0733\n","Epoch 2500: Loss 0.0715\n","Epoch 2600: Loss 0.0698\n","Epoch 2700: Loss 0.0682\n","Epoch 2800: Loss 0.0667\n","Epoch 2900: Loss 0.0653\n","Epoch 3000: Loss 0.0640\n","Epoch 3100: Loss 0.0627\n","Epoch 3200: Loss 0.0615\n","Epoch 3300: Loss 0.0604\n","Epoch 3400: Loss 0.0593\n","Epoch 3500: Loss 0.0583\n","Epoch 3600: Loss 0.0573\n","Epoch 3700: Loss 0.0564\n","Epoch 3800: Loss 0.0555\n","Epoch 3900: Loss 0.0546\n","Epoch 4000: Loss 0.0538\n","Epoch 4100: Loss 0.0530\n","Epoch 4200: Loss 0.0523\n","Epoch 4300: Loss 0.0515\n","Epoch 4400: Loss 0.0509\n","Epoch 4500: Loss 0.0502\n","Epoch 4600: Loss 0.0495\n","Epoch 4700: Loss 0.0489\n","Epoch 4800: Loss 0.0483\n","Epoch 4900: Loss 0.0477\n","Epoch 5000: Loss 0.0472\n","Epoch 5100: Loss 0.0466\n","Epoch 5200: Loss 0.0461\n","Epoch 5300: Loss 0.0456\n","Epoch 5400: Loss 0.0451\n","Epoch 5500: Loss 0.0446\n","Epoch 5600: Loss 0.0442\n","Epoch 5700: Loss 0.0437\n","Epoch 5800: Loss 0.0433\n","Epoch 5900: Loss 0.0428\n","Epoch 6000: Loss 0.0424\n","Epoch 6100: Loss 0.0420\n","Epoch 6200: Loss 0.0416\n","Epoch 6300: Loss 0.0412\n","Epoch 6400: Loss 0.0409\n","Epoch 6500: Loss 0.0405\n","Epoch 6600: Loss 0.0402\n","Epoch 6700: Loss 0.0398\n","Epoch 6800: Loss 0.0395\n","Epoch 6900: Loss 0.0392\n","Epoch 7000: Loss 0.0388\n","Epoch 7100: Loss 0.0385\n","Epoch 7200: Loss 0.0382\n","Epoch 7300: Loss 0.0379\n","Epoch 7400: Loss 0.0376\n","Epoch 7500: Loss 0.0373\n","Epoch 7600: Loss 0.0370\n","Epoch 7700: Loss 0.0368\n","Epoch 7800: Loss 0.0365\n","Epoch 7900: Loss 0.0362\n","Epoch 8000: Loss 0.0360\n","Epoch 8100: Loss 0.0357\n","Epoch 8200: Loss 0.0355\n","Epoch 8300: Loss 0.0352\n","Epoch 8400: Loss 0.0350\n","Epoch 8500: Loss 0.0348\n","Epoch 8600: Loss 0.0345\n","Epoch 8700: Loss 0.0343\n","Epoch 8800: Loss 0.0341\n","Epoch 8900: Loss 0.0339\n","Epoch 9000: Loss 0.0337\n","Epoch 9100: Loss 0.0335\n","Epoch 9200: Loss 0.0333\n","Epoch 9300: Loss 0.0331\n","Epoch 9400: Loss 0.0329\n","Epoch 9500: Loss 0.0327\n","Epoch 9600: Loss 0.0325\n","Epoch 9700: Loss 0.0323\n","Epoch 9800: Loss 0.0321\n","Epoch 9900: Loss 0.0319\n","Predictions for NOR gate: \n","[1 0 0 0]\n","\n","Training XOR gate:\n","Epoch 0: Loss 0.7502\n","Epoch 100: Loss 0.6952\n","Epoch 200: Loss 0.6940\n","Epoch 300: Loss 0.6935\n","Epoch 400: Loss 0.6933\n","Epoch 500: Loss 0.6932\n","Epoch 600: Loss 0.6932\n","Epoch 700: Loss 0.6932\n","Epoch 800: Loss 0.6932\n","Epoch 900: Loss 0.6932\n","Epoch 1000: Loss 0.6931\n","Epoch 1100: Loss 0.6931\n","Epoch 1200: Loss 0.6931\n","Epoch 1300: Loss 0.6931\n","Epoch 1400: Loss 0.6931\n","Epoch 1500: Loss 0.6931\n","Epoch 1600: Loss 0.6931\n","Epoch 1700: Loss 0.6931\n","Epoch 1800: Loss 0.6931\n","Epoch 1900: Loss 0.6931\n","Epoch 2000: Loss 0.6931\n","Epoch 2100: Loss 0.6931\n","Epoch 2200: Loss 0.6931\n","Epoch 2300: Loss 0.6931\n","Epoch 2400: Loss 0.6931\n","Epoch 2500: Loss 0.6931\n","Epoch 2600: Loss 0.6931\n","Epoch 2700: Loss 0.6931\n","Epoch 2800: Loss 0.6931\n","Epoch 2900: Loss 0.6931\n","Epoch 3000: Loss 0.6931\n","Epoch 3100: Loss 0.6931\n","Epoch 3200: Loss 0.6931\n","Epoch 3300: Loss 0.6931\n","Epoch 3400: Loss 0.6931\n","Epoch 3500: Loss 0.6931\n","Epoch 3600: Loss 0.6931\n","Epoch 3700: Loss 0.6931\n","Epoch 3800: Loss 0.6931\n","Epoch 3900: Loss 0.6931\n","Epoch 4000: Loss 0.6931\n","Epoch 4100: Loss 0.6931\n","Epoch 4200: Loss 0.6931\n","Epoch 4300: Loss 0.6931\n","Epoch 4400: Loss 0.6931\n","Epoch 4500: Loss 0.6931\n","Epoch 4600: Loss 0.6931\n","Epoch 4700: Loss 0.6931\n","Epoch 4800: Loss 0.6931\n","Epoch 4900: Loss 0.6931\n","Epoch 5000: Loss 0.6931\n","Epoch 5100: Loss 0.6931\n","Epoch 5200: Loss 0.6931\n","Epoch 5300: Loss 0.6931\n","Epoch 5400: Loss 0.6931\n","Epoch 5500: Loss 0.6931\n","Epoch 5600: Loss 0.6931\n","Epoch 5700: Loss 0.6931\n","Epoch 5800: Loss 0.6931\n","Epoch 5900: Loss 0.6931\n","Epoch 6000: Loss 0.6931\n","Epoch 6100: Loss 0.6931\n","Epoch 6200: Loss 0.6931\n","Epoch 6300: Loss 0.6931\n","Epoch 6400: Loss 0.6931\n","Epoch 6500: Loss 0.6931\n","Epoch 6600: Loss 0.6931\n","Epoch 6700: Loss 0.6931\n","Epoch 6800: Loss 0.6931\n","Epoch 6900: Loss 0.6931\n","Epoch 7000: Loss 0.6931\n","Epoch 7100: Loss 0.6931\n","Epoch 7200: Loss 0.6931\n","Epoch 7300: Loss 0.6931\n","Epoch 7400: Loss 0.6931\n","Epoch 7500: Loss 0.6931\n","Epoch 7600: Loss 0.6931\n","Epoch 7700: Loss 0.6931\n","Epoch 7800: Loss 0.6931\n","Epoch 7900: Loss 0.6931\n","Epoch 8000: Loss 0.6931\n","Epoch 8100: Loss 0.6931\n","Epoch 8200: Loss 0.6931\n","Epoch 8300: Loss 0.6931\n","Epoch 8400: Loss 0.6931\n","Epoch 8500: Loss 0.6931\n","Epoch 8600: Loss 0.6931\n","Epoch 8700: Loss 0.6931\n","Epoch 8800: Loss 0.6931\n","Epoch 8900: Loss 0.6931\n","Epoch 9000: Loss 0.6931\n","Epoch 9100: Loss 0.6931\n","Epoch 9200: Loss 0.6931\n","Epoch 9300: Loss 0.6931\n","Epoch 9400: Loss 0.6931\n","Epoch 9500: Loss 0.6931\n","Epoch 9600: Loss 0.6931\n","Epoch 9700: Loss 0.6931\n","Epoch 9800: Loss 0.6931\n","Epoch 9900: Loss 0.6931\n","Predictions for XOR gate: \n","[1 1 1 1]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kx7A3hvUj2J0","executionInfo":{"status":"ok","timestamp":1744306777448,"user_tz":-330,"elapsed":40,"user":{"displayName":"Onkar Jadhav","userId":"00426842055970906743"}}},"execution_count":11,"outputs":[]}]}